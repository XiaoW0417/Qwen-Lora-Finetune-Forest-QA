{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1,
      "grad_norm": 0.7513924241065979,
      "learning_rate": 9.946666666666668e-05,
      "loss": 3.0337,
      "step": 5
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.1605335474014282,
      "learning_rate": 9.88e-05,
      "loss": 3.1336,
      "step": 10
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.897844672203064,
      "learning_rate": 9.813333333333334e-05,
      "loss": 2.6994,
      "step": 15
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.9384863376617432,
      "learning_rate": 9.746666666666667e-05,
      "loss": 2.9132,
      "step": 20
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7235084772109985,
      "learning_rate": 9.680000000000001e-05,
      "loss": 2.7771,
      "step": 25
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.1328647136688232,
      "learning_rate": 9.613333333333334e-05,
      "loss": 2.2254,
      "step": 30
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.227874994277954,
      "learning_rate": 9.546666666666667e-05,
      "loss": 2.4597,
      "step": 35
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.6283634901046753,
      "learning_rate": 9.48e-05,
      "loss": 2.4678,
      "step": 40
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.477657675743103,
      "learning_rate": 9.413333333333334e-05,
      "loss": 2.4084,
      "step": 45
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4314874410629272,
      "learning_rate": 9.346666666666667e-05,
      "loss": 2.6354,
      "step": 50
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.2374943494796753,
      "learning_rate": 9.28e-05,
      "loss": 2.3072,
      "step": 55
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.3335131406784058,
      "learning_rate": 9.213333333333334e-05,
      "loss": 2.6883,
      "step": 60
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.7844642400741577,
      "learning_rate": 9.146666666666666e-05,
      "loss": 2.377,
      "step": 65
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.9122710824012756,
      "learning_rate": 9.080000000000001e-05,
      "loss": 2.0797,
      "step": 70
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.5239557027816772,
      "learning_rate": 9.013333333333333e-05,
      "loss": 1.9816,
      "step": 75
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.771356463432312,
      "learning_rate": 8.946666666666668e-05,
      "loss": 2.0606,
      "step": 80
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.4640604257583618,
      "learning_rate": 8.88e-05,
      "loss": 2.3892,
      "step": 85
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.5476086139678955,
      "learning_rate": 8.813333333333334e-05,
      "loss": 2.1987,
      "step": 90
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.540548324584961,
      "learning_rate": 8.746666666666667e-05,
      "loss": 2.3786,
      "step": 95
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2879812717437744,
      "learning_rate": 8.680000000000001e-05,
      "loss": 2.4607,
      "step": 100
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.3683947324752808,
      "learning_rate": 8.613333333333333e-05,
      "loss": 2.3615,
      "step": 105
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.1717796325683594,
      "learning_rate": 8.546666666666667e-05,
      "loss": 1.9863,
      "step": 110
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.6723660230636597,
      "learning_rate": 8.48e-05,
      "loss": 2.3339,
      "step": 115
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.93369460105896,
      "learning_rate": 8.413333333333334e-05,
      "loss": 2.1776,
      "step": 120
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.3910233974456787,
      "learning_rate": 8.346666666666667e-05,
      "loss": 2.061,
      "step": 125
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.7327240705490112,
      "learning_rate": 8.28e-05,
      "loss": 1.9793,
      "step": 130
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.0699594020843506,
      "learning_rate": 8.213333333333334e-05,
      "loss": 2.3438,
      "step": 135
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.9751098155975342,
      "learning_rate": 8.146666666666666e-05,
      "loss": 2.1813,
      "step": 140
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.258131980895996,
      "learning_rate": 8.080000000000001e-05,
      "loss": 2.3156,
      "step": 145
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.2901463508605957,
      "learning_rate": 8.013333333333333e-05,
      "loss": 2.0382,
      "step": 150
    },
    {
      "epoch": 3.1,
      "grad_norm": 1.634777307510376,
      "learning_rate": 7.946666666666667e-05,
      "loss": 1.985,
      "step": 155
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.60489821434021,
      "learning_rate": 7.88e-05,
      "loss": 2.4583,
      "step": 160
    },
    {
      "epoch": 3.3,
      "grad_norm": 2.055330276489258,
      "learning_rate": 7.813333333333334e-05,
      "loss": 1.9818,
      "step": 165
    },
    {
      "epoch": 3.4,
      "grad_norm": 2.77595853805542,
      "learning_rate": 7.746666666666666e-05,
      "loss": 1.9871,
      "step": 170
    },
    {
      "epoch": 3.5,
      "grad_norm": 2.1124520301818848,
      "learning_rate": 7.680000000000001e-05,
      "loss": 2.1295,
      "step": 175
    },
    {
      "epoch": 3.6,
      "grad_norm": 2.0035669803619385,
      "learning_rate": 7.613333333333333e-05,
      "loss": 2.047,
      "step": 180
    },
    {
      "epoch": 3.7,
      "grad_norm": 2.205692768096924,
      "learning_rate": 7.546666666666668e-05,
      "loss": 2.0765,
      "step": 185
    },
    {
      "epoch": 3.8,
      "grad_norm": 2.210376501083374,
      "learning_rate": 7.48e-05,
      "loss": 1.929,
      "step": 190
    },
    {
      "epoch": 3.9,
      "grad_norm": 2.0915400981903076,
      "learning_rate": 7.413333333333334e-05,
      "loss": 1.8949,
      "step": 195
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.6218678951263428,
      "learning_rate": 7.346666666666667e-05,
      "loss": 2.3338,
      "step": 200
    },
    {
      "epoch": 4.1,
      "grad_norm": 2.5652272701263428,
      "learning_rate": 7.280000000000001e-05,
      "loss": 1.939,
      "step": 205
    },
    {
      "epoch": 4.2,
      "grad_norm": 2.056720495223999,
      "learning_rate": 7.213333333333334e-05,
      "loss": 1.8616,
      "step": 210
    },
    {
      "epoch": 4.3,
      "grad_norm": 2.3338711261749268,
      "learning_rate": 7.146666666666666e-05,
      "loss": 2.2291,
      "step": 215
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.9844433069229126,
      "learning_rate": 7.08e-05,
      "loss": 2.2014,
      "step": 220
    },
    {
      "epoch": 4.5,
      "grad_norm": 3.365377187728882,
      "learning_rate": 7.013333333333333e-05,
      "loss": 2.0207,
      "step": 225
    },
    {
      "epoch": 4.6,
      "grad_norm": 2.2464239597320557,
      "learning_rate": 6.946666666666667e-05,
      "loss": 1.9329,
      "step": 230
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.087042808532715,
      "learning_rate": 6.879999999999999e-05,
      "loss": 2.0772,
      "step": 235
    },
    {
      "epoch": 4.8,
      "grad_norm": 2.028259515762329,
      "learning_rate": 6.813333333333334e-05,
      "loss": 2.197,
      "step": 240
    },
    {
      "epoch": 4.9,
      "grad_norm": 1.98020339012146,
      "learning_rate": 6.746666666666666e-05,
      "loss": 1.8031,
      "step": 245
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.09206223487854,
      "learning_rate": 6.680000000000001e-05,
      "loss": 1.9746,
      "step": 250
    },
    {
      "epoch": 5.1,
      "grad_norm": 2.788193941116333,
      "learning_rate": 6.613333333333333e-05,
      "loss": 1.9711,
      "step": 255
    },
    {
      "epoch": 5.2,
      "grad_norm": 2.609321117401123,
      "learning_rate": 6.546666666666667e-05,
      "loss": 2.0957,
      "step": 260
    },
    {
      "epoch": 5.3,
      "grad_norm": 2.0496551990509033,
      "learning_rate": 6.48e-05,
      "loss": 1.8583,
      "step": 265
    },
    {
      "epoch": 5.4,
      "grad_norm": 2.1343014240264893,
      "learning_rate": 6.413333333333334e-05,
      "loss": 1.8107,
      "step": 270
    },
    {
      "epoch": 5.5,
      "grad_norm": 2.2885916233062744,
      "learning_rate": 6.346666666666667e-05,
      "loss": 2.0359,
      "step": 275
    },
    {
      "epoch": 5.6,
      "grad_norm": 2.217933177947998,
      "learning_rate": 6.280000000000001e-05,
      "loss": 2.1266,
      "step": 280
    },
    {
      "epoch": 5.7,
      "grad_norm": 3.6473872661590576,
      "learning_rate": 6.213333333333333e-05,
      "loss": 1.6335,
      "step": 285
    },
    {
      "epoch": 5.8,
      "grad_norm": 1.7927113771438599,
      "learning_rate": 6.146666666666668e-05,
      "loss": 1.9693,
      "step": 290
    },
    {
      "epoch": 5.9,
      "grad_norm": 2.5773489475250244,
      "learning_rate": 6.08e-05,
      "loss": 1.8322,
      "step": 295
    },
    {
      "epoch": 6.0,
      "grad_norm": 3.1276724338531494,
      "learning_rate": 6.013333333333334e-05,
      "loss": 2.0267,
      "step": 300
    },
    {
      "epoch": 6.1,
      "grad_norm": 1.6910064220428467,
      "learning_rate": 5.946666666666667e-05,
      "loss": 1.9878,
      "step": 305
    },
    {
      "epoch": 6.2,
      "grad_norm": 2.9602677822113037,
      "learning_rate": 5.88e-05,
      "loss": 2.0655,
      "step": 310
    },
    {
      "epoch": 6.3,
      "grad_norm": 3.0963664054870605,
      "learning_rate": 5.813333333333334e-05,
      "loss": 1.7829,
      "step": 315
    },
    {
      "epoch": 6.4,
      "grad_norm": 2.8684659004211426,
      "learning_rate": 5.746666666666667e-05,
      "loss": 1.6916,
      "step": 320
    },
    {
      "epoch": 6.5,
      "grad_norm": 2.5553483963012695,
      "learning_rate": 5.68e-05,
      "loss": 1.6626,
      "step": 325
    },
    {
      "epoch": 6.6,
      "grad_norm": 3.7584240436553955,
      "learning_rate": 5.613333333333334e-05,
      "loss": 1.7522,
      "step": 330
    },
    {
      "epoch": 6.7,
      "grad_norm": 3.2659895420074463,
      "learning_rate": 5.546666666666667e-05,
      "loss": 2.0254,
      "step": 335
    },
    {
      "epoch": 6.8,
      "grad_norm": 3.740448474884033,
      "learning_rate": 5.4800000000000004e-05,
      "loss": 1.7196,
      "step": 340
    },
    {
      "epoch": 6.9,
      "grad_norm": 2.410090208053589,
      "learning_rate": 5.413333333333334e-05,
      "loss": 2.2916,
      "step": 345
    },
    {
      "epoch": 7.0,
      "grad_norm": 2.7535364627838135,
      "learning_rate": 5.346666666666667e-05,
      "loss": 1.8504,
      "step": 350
    },
    {
      "epoch": 7.1,
      "grad_norm": 3.105114221572876,
      "learning_rate": 5.28e-05,
      "loss": 1.6152,
      "step": 355
    },
    {
      "epoch": 7.2,
      "grad_norm": 2.7274649143218994,
      "learning_rate": 5.213333333333333e-05,
      "loss": 1.671,
      "step": 360
    },
    {
      "epoch": 7.3,
      "grad_norm": 2.853102684020996,
      "learning_rate": 5.146666666666667e-05,
      "loss": 1.7103,
      "step": 365
    },
    {
      "epoch": 7.4,
      "grad_norm": 4.339135646820068,
      "learning_rate": 5.08e-05,
      "loss": 1.771,
      "step": 370
    },
    {
      "epoch": 7.5,
      "grad_norm": 3.9007186889648438,
      "learning_rate": 5.013333333333333e-05,
      "loss": 1.7101,
      "step": 375
    },
    {
      "epoch": 7.6,
      "grad_norm": 2.8500521183013916,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 1.7095,
      "step": 380
    },
    {
      "epoch": 7.7,
      "grad_norm": 3.257863759994507,
      "learning_rate": 4.88e-05,
      "loss": 1.9365,
      "step": 385
    },
    {
      "epoch": 7.8,
      "grad_norm": 4.065990924835205,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 1.8218,
      "step": 390
    },
    {
      "epoch": 7.9,
      "grad_norm": 3.182274103164673,
      "learning_rate": 4.746666666666667e-05,
      "loss": 1.9489,
      "step": 395
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.9676483869552612,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 2.0874,
      "step": 400
    },
    {
      "epoch": 8.1,
      "grad_norm": 2.808056354522705,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 1.8716,
      "step": 405
    },
    {
      "epoch": 8.2,
      "grad_norm": 3.444537401199341,
      "learning_rate": 4.546666666666667e-05,
      "loss": 1.9143,
      "step": 410
    },
    {
      "epoch": 8.3,
      "grad_norm": 3.481580972671509,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 1.9026,
      "step": 415
    },
    {
      "epoch": 8.4,
      "grad_norm": 4.087373733520508,
      "learning_rate": 4.413333333333334e-05,
      "loss": 1.6312,
      "step": 420
    },
    {
      "epoch": 8.5,
      "grad_norm": 3.6713829040527344,
      "learning_rate": 4.346666666666667e-05,
      "loss": 1.8293,
      "step": 425
    },
    {
      "epoch": 8.6,
      "grad_norm": 3.7279958724975586,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 1.815,
      "step": 430
    },
    {
      "epoch": 8.7,
      "grad_norm": 4.1106414794921875,
      "learning_rate": 4.213333333333334e-05,
      "loss": 1.9657,
      "step": 435
    },
    {
      "epoch": 8.8,
      "grad_norm": 3.6982085704803467,
      "learning_rate": 4.146666666666667e-05,
      "loss": 1.6096,
      "step": 440
    },
    {
      "epoch": 8.9,
      "grad_norm": 4.028094291687012,
      "learning_rate": 4.08e-05,
      "loss": 1.6356,
      "step": 445
    },
    {
      "epoch": 9.0,
      "grad_norm": 2.5747320652008057,
      "learning_rate": 4.013333333333333e-05,
      "loss": 1.5461,
      "step": 450
    },
    {
      "epoch": 9.1,
      "grad_norm": 4.1754045486450195,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 1.7371,
      "step": 455
    },
    {
      "epoch": 9.2,
      "grad_norm": 3.6629209518432617,
      "learning_rate": 3.88e-05,
      "loss": 1.6508,
      "step": 460
    },
    {
      "epoch": 9.3,
      "grad_norm": 3.107635498046875,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 1.6458,
      "step": 465
    },
    {
      "epoch": 9.4,
      "grad_norm": 4.314279556274414,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 1.5576,
      "step": 470
    },
    {
      "epoch": 9.5,
      "grad_norm": 4.06713342666626,
      "learning_rate": 3.68e-05,
      "loss": 1.6733,
      "step": 475
    },
    {
      "epoch": 9.6,
      "grad_norm": 5.027483940124512,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 2.0567,
      "step": 480
    },
    {
      "epoch": 9.7,
      "grad_norm": 3.9851112365722656,
      "learning_rate": 3.546666666666667e-05,
      "loss": 1.6454,
      "step": 485
    },
    {
      "epoch": 9.8,
      "grad_norm": 4.0833048820495605,
      "learning_rate": 3.48e-05,
      "loss": 1.7808,
      "step": 490
    },
    {
      "epoch": 9.9,
      "grad_norm": 4.075794696807861,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 1.705,
      "step": 495
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.203881740570068,
      "learning_rate": 3.346666666666667e-05,
      "loss": 1.6304,
      "step": 500
    },
    {
      "epoch": 10.1,
      "grad_norm": 3.5149574279785156,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 1.9172,
      "step": 505
    },
    {
      "epoch": 10.2,
      "grad_norm": 3.0069868564605713,
      "learning_rate": 3.213333333333334e-05,
      "loss": 1.4839,
      "step": 510
    },
    {
      "epoch": 10.3,
      "grad_norm": 3.7363176345825195,
      "learning_rate": 3.146666666666667e-05,
      "loss": 1.6747,
      "step": 515
    },
    {
      "epoch": 10.4,
      "grad_norm": 3.5999467372894287,
      "learning_rate": 3.08e-05,
      "loss": 1.5628,
      "step": 520
    },
    {
      "epoch": 10.5,
      "grad_norm": 4.6137518882751465,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 1.8213,
      "step": 525
    },
    {
      "epoch": 10.6,
      "grad_norm": 4.484592437744141,
      "learning_rate": 2.946666666666667e-05,
      "loss": 1.4901,
      "step": 530
    },
    {
      "epoch": 10.7,
      "grad_norm": 4.029736042022705,
      "learning_rate": 2.88e-05,
      "loss": 1.8282,
      "step": 535
    },
    {
      "epoch": 10.8,
      "grad_norm": 4.273346424102783,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 1.6854,
      "step": 540
    },
    {
      "epoch": 10.9,
      "grad_norm": 4.866912364959717,
      "learning_rate": 2.746666666666667e-05,
      "loss": 1.7038,
      "step": 545
    },
    {
      "epoch": 11.0,
      "grad_norm": 4.17981481552124,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 1.6438,
      "step": 550
    },
    {
      "epoch": 11.1,
      "grad_norm": 2.601987838745117,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 1.9339,
      "step": 555
    },
    {
      "epoch": 11.2,
      "grad_norm": 4.984426975250244,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 1.6631,
      "step": 560
    },
    {
      "epoch": 11.3,
      "grad_norm": 2.4791667461395264,
      "learning_rate": 2.48e-05,
      "loss": 1.577,
      "step": 565
    },
    {
      "epoch": 11.4,
      "grad_norm": 4.450832366943359,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 1.3926,
      "step": 570
    },
    {
      "epoch": 11.5,
      "grad_norm": 5.3748393058776855,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 1.6896,
      "step": 575
    },
    {
      "epoch": 11.6,
      "grad_norm": 4.7779860496521,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 1.5115,
      "step": 580
    },
    {
      "epoch": 11.7,
      "grad_norm": 5.6071624755859375,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 2.035,
      "step": 585
    },
    {
      "epoch": 11.8,
      "grad_norm": 3.7842226028442383,
      "learning_rate": 2.146666666666667e-05,
      "loss": 1.252,
      "step": 590
    },
    {
      "epoch": 11.9,
      "grad_norm": 5.258066177368164,
      "learning_rate": 2.08e-05,
      "loss": 1.8018,
      "step": 595
    },
    {
      "epoch": 12.0,
      "grad_norm": 3.1142914295196533,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 1.6168,
      "step": 600
    },
    {
      "epoch": 12.1,
      "grad_norm": 3.0686419010162354,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 1.502,
      "step": 605
    },
    {
      "epoch": 12.2,
      "grad_norm": 4.147475242614746,
      "learning_rate": 1.88e-05,
      "loss": 1.3262,
      "step": 610
    },
    {
      "epoch": 12.3,
      "grad_norm": 4.775862216949463,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 1.4901,
      "step": 615
    },
    {
      "epoch": 12.4,
      "grad_norm": 6.234807014465332,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 1.7157,
      "step": 620
    },
    {
      "epoch": 12.5,
      "grad_norm": 5.092072010040283,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.7728,
      "step": 625
    },
    {
      "epoch": 12.6,
      "grad_norm": 5.112246990203857,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 1.7515,
      "step": 630
    },
    {
      "epoch": 12.7,
      "grad_norm": 4.601887226104736,
      "learning_rate": 1.546666666666667e-05,
      "loss": 1.7247,
      "step": 635
    },
    {
      "epoch": 12.8,
      "grad_norm": 4.782191753387451,
      "learning_rate": 1.48e-05,
      "loss": 1.7431,
      "step": 640
    },
    {
      "epoch": 12.9,
      "grad_norm": 4.433506488800049,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 1.4161,
      "step": 645
    },
    {
      "epoch": 13.0,
      "grad_norm": 5.039186954498291,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 1.5936,
      "step": 650
    },
    {
      "epoch": 13.1,
      "grad_norm": 3.841625928878784,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.4376,
      "step": 655
    },
    {
      "epoch": 13.2,
      "grad_norm": 3.9254653453826904,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 1.7203,
      "step": 660
    },
    {
      "epoch": 13.3,
      "grad_norm": 5.220774173736572,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 1.3386,
      "step": 665
    },
    {
      "epoch": 13.4,
      "grad_norm": 5.462586879730225,
      "learning_rate": 1.08e-05,
      "loss": 1.7626,
      "step": 670
    },
    {
      "epoch": 13.5,
      "grad_norm": 4.413996696472168,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 1.9979,
      "step": 675
    },
    {
      "epoch": 13.6,
      "grad_norm": 3.9629461765289307,
      "learning_rate": 9.466666666666667e-06,
      "loss": 1.4412,
      "step": 680
    },
    {
      "epoch": 13.7,
      "grad_norm": 4.778079032897949,
      "learning_rate": 8.8e-06,
      "loss": 1.5921,
      "step": 685
    },
    {
      "epoch": 13.8,
      "grad_norm": 5.084006309509277,
      "learning_rate": 8.133333333333332e-06,
      "loss": 1.6006,
      "step": 690
    },
    {
      "epoch": 13.9,
      "grad_norm": 4.298445224761963,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 1.6356,
      "step": 695
    },
    {
      "epoch": 14.0,
      "grad_norm": 5.520633697509766,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.3102,
      "step": 700
    },
    {
      "epoch": 14.1,
      "grad_norm": 5.431220054626465,
      "learning_rate": 6.133333333333334e-06,
      "loss": 1.5246,
      "step": 705
    },
    {
      "epoch": 14.2,
      "grad_norm": 2.6703903675079346,
      "learning_rate": 5.466666666666667e-06,
      "loss": 1.5989,
      "step": 710
    },
    {
      "epoch": 14.3,
      "grad_norm": 5.213386058807373,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.546,
      "step": 715
    },
    {
      "epoch": 14.4,
      "grad_norm": 4.736680030822754,
      "learning_rate": 4.133333333333333e-06,
      "loss": 1.61,
      "step": 720
    },
    {
      "epoch": 14.5,
      "grad_norm": 4.336584568023682,
      "learning_rate": 3.466666666666667e-06,
      "loss": 1.4508,
      "step": 725
    },
    {
      "epoch": 14.6,
      "grad_norm": 4.133192539215088,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.6766,
      "step": 730
    },
    {
      "epoch": 14.7,
      "grad_norm": 3.9531002044677734,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 1.5338,
      "step": 735
    },
    {
      "epoch": 14.8,
      "grad_norm": 5.224938869476318,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 1.5941,
      "step": 740
    },
    {
      "epoch": 14.9,
      "grad_norm": 3.8811256885528564,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.8697,
      "step": 745
    },
    {
      "epoch": 15.0,
      "grad_norm": 4.734748840332031,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 1.3929,
      "step": 750
    }
  ],
  "logging_steps": 5,
  "max_steps": 750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3443979640504320.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
